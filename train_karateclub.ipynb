{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from src.data import get_graph, split\n",
    "from exp_ntk_certify_collective_label import run, configure_hardware\n",
    "from src.models.common import row_normalize, sym_normalize\n",
    "from src.models import create_model\n",
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2\n",
    "\n",
    "data_params = dict(\n",
    "    dataset = \"karate_club\",    \n",
    "    learning_setting = \"transductive\", # or \"transdructive\"\n",
    "    specification = dict(\n",
    "        n_per_class = 2,\n",
    "        fraction_test = 0.1,\n",
    "        data_dir = \"./data\",\n",
    "        balance_test = True,\n",
    "        seed = 2,\n",
    "    )\n",
    ")\n",
    "\n",
    "model_params = dict(\n",
    "    label = \"SGC\",\n",
    "    model = \"GCN\",\n",
    "    normalization = \"row_normalization\",\n",
    "    depth = 1,\n",
    "    regularizer = 0.01,\n",
    "    pred_method = \"svm\",\n",
    "    activation = \"linear\",\n",
    "    solver = \"qplayer\",\n",
    "    alpha_tol = 1e-4,\n",
    "    bias = False,\n",
    ")\n",
    "\n",
    "model_params_finite = dict(\n",
    "    model = \"GCN\",\n",
    "    normalization = \"row_normalization\",\n",
    "    activation = \"linear\",\n",
    "    depth = 1,\n",
    "    n_filter = 8,\n",
    ")\n",
    "\n",
    "certificate_params = dict(\n",
    "    delta = 0.2,\n",
    "    LogToConsole = 0,\n",
    "    OutputFlag = 1,\n",
    "    IntegralityFocus = 1,\n",
    "    use_tight_big_M = 1\n",
    ")\n",
    "\n",
    "verbosity_params = dict(\n",
    "    debug_lvl = \"info\"\n",
    ")  \n",
    "\n",
    "other_params = dict(\n",
    "    device = \"cpu\",\n",
    "    dtype = torch.float64,\n",
    "    allow_tf32 = False,\n",
    "    path_gurobi_license = \"/ceph/ssd/staff/gosl/app/gurobi.lic\"\n",
    ")\n",
    "n_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 19:44:15 (INFO): Starting experiment exp_ntk_certify_collective_label with configuration:\n",
      "2024-09-06 19:44:15 (INFO): data_params: {'dataset': 'karate_club', 'learning_setting': 'transductive', 'specification': {'n_per_class': 2, 'fraction_test': 0.1, 'data_dir': './data', 'balance_test': True, 'seed': 2}}\n",
      "2024-09-06 19:44:15 (INFO): model_params: {'label': 'SGC', 'model': 'GCN', 'normalization': 'row_normalization', 'depth': 1, 'regularizer': 0.01, 'pred_method': 'svm', 'activation': 'linear', 'solver': 'qplayer', 'alpha_tol': 0.0001, 'bias': False}\n",
      "2024-09-06 19:44:15 (INFO): certification_params: {'delta': 0.2, 'LogToConsole': 0, 'OutputFlag': 1, 'IntegralityFocus': 1, 'use_tight_big_M': 1}\n",
      "2024-09-06 19:44:15 (INFO): verbosity_params: {'debug_lvl': 'info'}\n",
      "2024-09-06 19:44:15 (INFO): other_params: {'device': 'cpu', 'dtype': torch.float32, 'allow_tf32': False, 'path_gurobi_license': '/ceph/ssd/staff/gosl/app/gurobi.lic'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 19:44:15 (INFO): seed: 2\n",
      "2024-09-06 19:44:15 (INFO): number of samples\n",
      " - labeled: 4 \n",
      " - val: 4 \n",
      " - test: 4 \n",
      " - unlabeled: 22\n",
      "2024-09-06 19:44:15 (INFO): Delta: 0.2\n",
      "2024-09-06 19:44:15 (INFO): Test accuracy: 0.9615384340286255\n",
      "2024-09-06 19:44:15 (INFO): Train accuracy: 1.0\n",
      "2024-09-06 19:44:15 (INFO): Using tight big-Ms.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 alphas found: ['0.0100', '0.0100', '0.0100', '0.0100', '0.0100', '0.0100', '0.0100', '0.0100']\n",
      "Warning for adding constraints: zero or small (< 1e-13) coefficients, ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 19:44:15 (INFO): Warning for adding constraints: zero or small (< 1e-13) coefficients, ignored\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter IntegralityFocus to value 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 19:44:15 (INFO): Set parameter IntegralityFocus to value 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter IntFeasTol to value 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-06 19:44:15 (INFO): Set parameter IntFeasTol to value 0.0001\n",
      "2024-09-06 19:44:15 (INFO): Optimization status: 2\n",
      "2024-09-06 19:44:15 (INFO): Objective: #sign flips 5.0 out of 26\n",
      "2024-09-06 19:44:15 (INFO): Percentage of nodes certified 0.8076923076923077\n",
      "2024-09-06 19:44:15 (INFO): Certified accuracy (poisoning): 0.8076923076923077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded user MIP start with objective -0\n",
      "\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0]\n",
      "[1, 1, 1, 1, 1, 1, -1, 1, 1, 1, -1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, -1]\n",
      "[-0.0, 1.0, 1.0, 1.0, -0.0, -0.0, 1.0, 1.0]\n",
      "[23, 32, 5, 19, 27, 24, 7, 3]\n",
      "Epoch 0: loss=0.667255163192749\n",
      "Epoch 20: loss=0.34523576498031616\n",
      "Epoch 40: loss=0.11127869039773941\n",
      "Epoch 60: loss=0.03858467563986778\n",
      "Epoch 80: loss=0.02004377357661724\n",
      "Clean: Acc.Trn.: 1.00 Acc.Tst.: 0.92\n",
      "Epoch 0: loss=0.6753969192504883\n",
      "Epoch 20: loss=0.2727285921573639\n",
      "Epoch 40: loss=0.08812858909368515\n",
      "Epoch 60: loss=0.03193012252449989\n",
      "Epoch 80: loss=0.016871007159352303\n",
      "Perturbed: Acc.Trn.: 1.00 Acc.Tst.: 0.27\n",
      "7 Robust Empirical (0.27%)\n",
      "21 Robust in NTK (0.81%)\n",
      "of which 0.19% still robust for finite width\n",
      "0.60% of NTK-non robust is robust for finite width\n",
      "tensor([1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1,\n",
      "        1, 1])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "# Get collective certificate\n",
    "result = run(data_params, model_params, certificate_params, verbosity_params, other_params, seed)\n",
    "idx_train, idx_val, = result[\"idx_train\"], result[\"idx_val\"]\n",
    "idx_labeled, idx_test = result[\"idx_labeled\"], result[\"idx_test\"]\n",
    "acc_tst_wide = result[\"accuracy_test\"]\n",
    "print(result[\"y_is_robust\"])\n",
    "print(result[\"y_true_cls\"])\n",
    "print(result[\"y_flip\"])\n",
    "print(result[\"idx_labeled\"])\n",
    "# reset/configure hardware\n",
    "other_params[\"dtype\"] = torch.float32\n",
    "device, dtype = configure_hardware(other_params, seed)\n",
    "rng = np.random.Generator(np.random.PCG64(seed))\n",
    "# get graph\n",
    "X, A, y, _, _, _ = get_graph(data_params, sort=True)\n",
    "X = torch.tensor(X, dtype=dtype, device=device)\n",
    "A = torch.tensor(A, dtype=dtype, device=device)\n",
    "y = torch.tensor(y, dtype=dtype, device=device)\n",
    "if \"normalization\" in model_params:\n",
    "    if model_params[\"normalization\"] == \"row_normalization\":\n",
    "        A = row_normalize(A)\n",
    "    else:\n",
    "        A = sym_normalize(A)\n",
    "y_pert = y.clone()\n",
    "y_flip = torch.tensor(result[\"y_flip\"], dtype=dtype, device=device)\n",
    "y_pert[idx_labeled] = y_pert[idx_labeled].logical_xor(y_flip).float()\n",
    "for i, y in enumerate([y, y_pert]):\n",
    "    # init model\n",
    "    model_params_finite[\"n_classes\"] = len(np.unique(y))\n",
    "    model_params_finite[\"n_features\"] = X.shape[1]\n",
    "    model = create_model(model_params_finite)\n",
    "    model = model.to(device)\n",
    "    # train model\n",
    "    model.train()\n",
    "    loss_f = torch.nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(X, A).reshape(-1)\n",
    "        loss = loss_f(logits[idx_labeled], y[idx_labeled])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}: loss={loss.item()}\")\n",
    "    pred = model(X, A).reshape(-1)\n",
    "    y_pred = (pred > 0.5).float()\n",
    "    acc_trn = (y_pred[idx_labeled] == y[idx_labeled]).float().mean()\n",
    "    acc_tst = (y_pred[idx_test] == y[idx_test]).float().mean()\n",
    "    if i == 0:\n",
    "        print(f\"Clean: Acc.Trn.: {acc_trn:.2f} Acc.Tst.: {acc_tst:.2f}\")\n",
    "        y_pred_cln = y_pred\n",
    "    else:\n",
    "        print(f\"Perturbed: Acc.Trn.: {acc_trn:.2f} Acc.Tst.: {acc_tst:.2f}\")\n",
    "        y_pred_pert = y_pred\n",
    "y_is_robust = torch.tensor(result[\"y_is_robust\"], dtype=torch.long, device=device)\n",
    "y_is_robust_emp = (y_pred_cln[idx_test] == y_pred_pert[idx_test]).long()\n",
    "tst_robust = y_is_robust_emp.float().mean()\n",
    "tst_robust_c = y_is_robust_emp.sum()\n",
    "print(f\"{tst_robust_c} Robust Empirical ({tst_robust:.2f}%)\")\n",
    "y_is_robust_c = y_is_robust.sum()\n",
    "y_is_robust_mask = y_is_robust == 1\n",
    "y_both_robust_c = (y_is_robust_emp[y_is_robust_mask] == y_is_robust[y_is_robust_mask]).sum()\n",
    "print(f\"{y_is_robust_c} Robust in NTK ({y_is_robust.float().mean():.2f}%)\")\n",
    "print(f\"of which {y_both_robust_c / y_is_robust[y_is_robust_mask].sum():.2f}% still robust for finite width\")\n",
    "# Robust Empirical but not Robust NTK?\n",
    "y_not_robust_mask = y_is_robust == 0\n",
    "y_diff_robust_c = (y_is_robust_emp[y_not_robust_mask] != y_is_robust[y_not_robust_mask]).sum()\n",
    "print(f\"{y_diff_robust_c / len(y_is_robust[y_not_robust_mask]):.2f}% of NTK-non robust is robust for finite width\")\n",
    "print(f\"{y_is_robust}\")\n",
    "print(f\"{y_is_robust_emp}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_ntk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
