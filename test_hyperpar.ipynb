{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NTK hyperparams search on sbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from exp_ntk_hyperparam import run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_str_l(l, precision=2):\n",
    "    l_str = []\n",
    "    for el in l:\n",
    "        l_str.append(f\"{el:.{precision}f}\")\n",
    "    return l_str\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "data_params = dict(\n",
    "    dataset = \"csbm\",\n",
    "    learning_setting = \"transductive\", # or \"transdructive\"\n",
    "    cv_folds = 4,\n",
    "    specification = dict(\n",
    "        classes = 2,\n",
    "        n_trn_labeled = 40, \n",
    "        n_trn_unlabeled = 0,\n",
    "        n_val = 40,\n",
    "        n_test = 120,\n",
    "        sigma = 1,\n",
    "        avg_within_class_degree = 1.58 * 2,\n",
    "        avg_between_class_degree = 0.37 * 2,\n",
    "        K = 1.5,\n",
    "        seed = 0 # used to generate the dataset & data split\n",
    "    )\n",
    ")\n",
    "\n",
    "model_params = dict(\n",
    "    label = \"GCN\",\n",
    "    model = \"GCN\",\n",
    "    normalization = \"row_normalization\",\n",
    "    depth = 1,\n",
    "    #regularizer = 1e-8\n",
    "    regularizer = 1,\n",
    "    pred_method = \"svm\",\n",
    "    activation = \"relu\",\n",
    "    solver = \"qplayer_one_vs_all\",\n",
    "    alpha_tol = 1e-4,\n",
    "    bias = False,\n",
    ")\n",
    "\n",
    "verbosity_params = dict(\n",
    "    debug_lvl = \"warning\"\n",
    ")  \n",
    "\n",
    "other_params = dict(\n",
    "    device = \"0\",\n",
    "    dtype = \"float64\",\n",
    "    allow_tf32 = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = dict(\n",
    "    dataset = \"us_county\",    \n",
    "    learning_setting = \"transductive\", # or \"transdructive\"\n",
    "    cv_folds = 4,\n",
    "    specification = dict(\n",
    "        year=2012,    \n",
    "        n_per_class = 50,\n",
    "        fraction_test = 0.1,\n",
    "        data_dir = \"./data\",\n",
    "        make_undirected = True,\n",
    "        binary_attr = False,\n",
    "        balance_test = True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = dict(\n",
    "    dataset = \"cora_ml_cont_binary\",\n",
    "    learning_setting = \"transductive\", # or \"transdructive\"\n",
    "    specification = dict(\n",
    "        n_per_class = 3,\n",
    "        fraction_test = 0.1,\n",
    "        data_dir = \"./data\",\n",
    "        make_undirected = True,\n",
    "        binary_attr = False,\n",
    "        balance_test = True,\n",
    "    )\n",
    ")\n",
    "model_params = dict(\n",
    "    label = \"APPNP\",\n",
    "    model = \"APPNP\",\n",
    "    #model = \"GCN\",\n",
    "    normalization = \"sym_normalization\",\n",
    "    iteration=10,\n",
    "    alpha=1,\n",
    "    depth = 1,\n",
    "    #regularizer = 1e-8\n",
    "    pred_method = \"svm\",\n",
    "    activation = \"relu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = dict(\n",
    "    dataset = \"wikics_binary\",\n",
    "    learning_setting = \"transductive\", # or \"transdructive\"\n",
    "    cv_folds = 4,\n",
    "    specification = dict(\n",
    "        n_per_class = 25,\n",
    "        fraction_test = 0.1,\n",
    "        data_dir = \"./data\",\n",
    "        make_undirected = True,\n",
    "        binary_attr = False,\n",
    "        balance_test = True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = dict(\n",
    "    dataset = \"polblogs\",\n",
    "    learning_setting = \"transductive\", # or \"transdructive\"\n",
    "    cv_folds = 4,\n",
    "    specification = dict(\n",
    "        n_per_class = 5,\n",
    "        fraction_test = 0.1,\n",
    "        data_dir = \"./data\",\n",
    "        make_undirected = True,\n",
    "        binary_attr = False,\n",
    "        balance_test = True,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_exp(n_seeds, data_params, model_params, verbosity_params, other_params, n_seed_l=None):\n",
    "    acc_l = []\n",
    "    min_ypred = []\n",
    "    max_ypred = []\n",
    "    cond = []\n",
    "    min_ntklabeled = []\n",
    "    max_ntklabeled = []\n",
    "    min_ntkunlabeled = []\n",
    "    max_ntkunlabeled = []\n",
    "    if n_seed_l is None:\n",
    "        seeds = [seed for seed in range(n_seeds)]\n",
    "    else:\n",
    "        seeds = n_seed_l\n",
    "    for seed in seeds:\n",
    "        data_params[\"specification\"][\"seed\"] = seed\n",
    "        res = run(data_params, model_params, verbosity_params, other_params, seed)\n",
    "        acc_l.append(np.mean(res[\"val_acc_l\"]))\n",
    "        #min_ypred.append(res[\"min_ypred\"])\n",
    "        #max_ypred.append(res[\"max_ypred\"])\n",
    "        #min_ntklabeled.append(res[\"min_ntklabeled\"])\n",
    "        #max_ntklabeled.append(res[\"max_ntklabeled\"])\n",
    "        #min_ntkunlabeled.append(res[\"min_ntkunlabeled\"])\n",
    "        #max_ntkunlabeled.append(res[\"max_ntkunlabeled\"])\n",
    "        #cond.append(res[\"cond\"])\n",
    "    print(f\"Accuracy: {get_str_l(acc_l)}\")\n",
    "    print(f\"Accuracy Mean: {np.array(acc_l).mean()}\")\n",
    "    print(f\"Accuracy Std: {np.array(acc_l).std()}\")\n",
    "    #print(f\"Min y_pred: {get_str_l(min_ypred)}\")\n",
    "    #print(f\"Max y_pred: {get_str_l(max_ypred)}\")\n",
    "    #print(f\"Min NTK_labeled: {get_str_l(min_ntklabeled)}\")\n",
    "    #print(f\"Max NTK_labeled: {get_str_l(max_ntklabeled)}\")\n",
    "    #print(f\"Min NTK_unlabeled: {get_str_l(min_ntkunlabeled)}\")\n",
    "    #print(f\"Max NTK_unlabeled: {get_str_l(max_ntkunlabeled)}\")\n",
    "    #print(f\"Condition: {get_str_l(cond, precision=0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = dict(\n",
    "    dataset = \"citeseer\",\n",
    "    learning_setting = \"transductive\", # or \"transdructive\"\n",
    "    cv_folds = 4,\n",
    "    specification = dict(\n",
    "        n_per_class = 10,\n",
    "        fraction_test = 0.1,\n",
    "        data_dir = \"./data\",\n",
    "        make_undirected = True,\n",
    "        binary_attr = False,\n",
    "        balance_test = True,\n",
    "    )\n",
    ")\n",
    "model_params = dict(\n",
    "    label = \"GCN\",\n",
    "    model = \"GCN\",\n",
    "    normalization = \"row_normalization\",\n",
    "    depth = 1,\n",
    "    #regularizer = 1e-8\n",
    "    regularizer = 1,\n",
    "    pred_method = \"svm\",\n",
    "    activation = \"relu\",\n",
    "    solver = \"qplayer_one_vs_all\",\n",
    "    alpha_tol = 1e-4,\n",
    "    bias = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 1: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 2: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 3: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 4: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 5: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 0: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 1: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 2: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 3: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 4: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 5: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 0: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 1: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 2: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 3: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 4: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 5: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 0: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 1: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 2: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 3: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 4: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 5: 75 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 0: 100 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 1: 100 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 2: 100 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 3: 100 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 4: 100 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n",
      "Class 5: 100 alphas found: ['0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001', '0.0001']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'trn_acc': 0.875,\n",
       " 'val_acc_l': [0.800000011920929,\n",
       "  0.5333333611488342,\n",
       "  0.6000000238418579,\n",
       "  0.699999988079071],\n",
       " 'test_acc': 0.7045226097106934,\n",
       " 'trn_min_ypred': -0.07280606776475906,\n",
       " 'trn_max_ypred': -0.039096154272556305,\n",
       " 'trn_min_ntkunlabeled': 2.772995569145558,\n",
       " 'trn_max_ntkunlabeled': 38.32873519711948,\n",
       " 'val_cond': [828.5800415639267,\n",
       "  8859.244808814501,\n",
       "  661.615331119383,\n",
       "  8838.475962699571],\n",
       " 'val_min_ypred': [-0.05129873752593994,\n",
       "  -0.05109310895204544,\n",
       "  -0.05245276540517807,\n",
       "  -0.05079745873808861],\n",
       " 'val_max_ypred': [-0.028744854032993317,\n",
       "  -0.03298565372824669,\n",
       "  -0.032100412994623184,\n",
       "  -0.03142448142170906],\n",
       " 'val_min_ntklabeled': [3.2258991856536015,\n",
       "  2.772995569145558,\n",
       "  3.0239330420499213,\n",
       "  2.772995569145558],\n",
       " 'val_max_ntklabeled': [32.81241245865693,\n",
       "  38.32883519711948,\n",
       "  38.32883519711948,\n",
       "  38.32883519711948],\n",
       " 'val_min_ntkunlabeled': [2.5801716245896613,\n",
       "  2.5801716245896613,\n",
       "  2.5801716245896613,\n",
       "  2.5801716245896613],\n",
       " 'val_max_ntkunlabeled': [34.78775442573233,\n",
       "  34.78775442573233,\n",
       "  34.78775442573233,\n",
       "  34.78775442573233],\n",
       " 'test_min_ypred': -0.08353366702795029,\n",
       " 'test_max_ypred': -0.039016738533973694,\n",
       " 'test_min_ntklabeled': 2.772995569145558,\n",
       " 'test_max_ntklabeled': 38.32883519711948,\n",
       " 'test_min_ntkunlabeled': 2.5801716245896613,\n",
       " 'test_max_ntkunlabeled': 34.78775442573233,\n",
       " 'test_cond': 11726.75615533225}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params[\"regularizer\"] = 0.0001\n",
    "model_params[\"solver\"] = \"qplayer_one_vs_all\"\n",
    "model_params[\"alpha_tol\"] = 1e-4\n",
    "model_params[\"bias\"] = False\n",
    "data_params[\"learning_setting\"] = \"transductive\"\n",
    "model_params[\"pred_method\"] = \"svm\"\n",
    "model_params[\"cache_size\"] = 10000\n",
    "other_params[\"device\"] = \"cpu\"\n",
    "run(data_params, model_params, verbosity_params, other_params, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = dict(\n",
    "    label = \"GCN_skipalpha\",\n",
    "    model = \"GCN\",\n",
    "    normalization = \"row_normalization\",\n",
    "    depth = 1,\n",
    "    #regularizer = 1e-8\n",
    "    regularizer = 1,\n",
    "    pred_method = \"svm\",\n",
    "    activation = \"relu\",\n",
    "    solver = \"qplayer_one_vs_all\",\n",
    "    alpha_tol = 1e-4,\n",
    "    bias = False,\n",
    ")\n",
    "model_params[\"regularizer\"] = 0.0001\n",
    "model_params[\"solver\"] = \"qplayer_one_vs_all\"\n",
    "model_params[\"alpha_tol\"] = 1e-4\n",
    "model_params[\"bias\"] = False\n",
    "data_params[\"learning_setting\"] = \"transductive\"\n",
    "model_params[\"pred_method\"] = \"svm\"\n",
    "model_params[\"cache_size\"] = 10000\n",
    "other_params[\"device\"] = \"cpu\"\n",
    "run(data_params, model_params, verbosity_params, other_params, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = dict(\n",
    "    dataset = \"polblogs\",\n",
    "    learning_setting = \"transductive\", # or \"transdructive\"\n",
    "    cv_folds = 4,\n",
    "    specification = dict(\n",
    "        n_per_class = 5,\n",
    "        fraction_test = 0.1,\n",
    "        data_dir = \"./data\",\n",
    "        make_undirected = True,\n",
    "        binary_attr = False,\n",
    "        balance_test = True,\n",
    "    )\n",
    ")\n",
    "model_params = dict(\n",
    "    label = \"APPNP\",\n",
    "    model = \"APPNP\",\n",
    "    normalization = \"sym_normalization\",\n",
    "    alpha = 0.2,\n",
    "    iteration = 10,\n",
    "    depth = 1,\n",
    "    #regularizer = 1e-8\n",
    "    regularizer = 0.5,\n",
    "    pred_method = \"svm\",\n",
    "    activation = \"relu\"\n",
    ")\n",
    "\n",
    "model_params[\"regularizer\"] = 20\n",
    "model_params[\"solver\"] = \"qplayer\"\n",
    "model_params[\"alpha_tol\"] = 1e-4\n",
    "model_params[\"bias\"] = False\n",
    "data_params[\"learning_setting\"] = \"transductive\"\n",
    "model_params[\"pred_method\"] = \"svm\"\n",
    "model_params[\"cache_size\"] = 10000\n",
    "other_params[\"device\"] = \"cpu\"\n",
    "#run(data_params, model_params, verbosity_params, other_params, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ceph/ssd/staff/gosl/.conda/envs/py311_ntk/lib/python3.11/site-packages/torch_geometric/datasets/wikics.py:38: UserWarning: The WikiCS dataset now returns an undirected graph by default. Please explicitly specify 'is_undirected=False' to restore the old behavior.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 46 alphas found: ['0.0000', '1.0000', '0.3594', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.1864', '-0.0000', '-0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.1472', '-0.0000', '0.0000', '-0.0000', '1.0000', '1.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0446', '-0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '0.0000', '0.6486', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '0.0000', '0.0000', '0.0275', '-0.0000', '0.0000', '-0.0000', '0.9801', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0835', '0.0000', '0.1229', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.7265', '0.0000', '1.0000', '0.0000', '-0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '0.5215', '1.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '1.0000', '0.0000', '-0.0000', '0.2962', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '0.0000', '1.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.1492', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '1.0000', '0.0000', '0.0000', '-0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.3691', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.7588', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '0.0493', '-0.0000', '0.0065', '-0.0000', '0.0000', '-0.0000', '1.0000', '0.0000']\n",
      "Class 1: 46 alphas found: ['-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0451', '1.0000', '1.0000', '0.9633', '1.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '1.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '1.0000', '1.0000', '0.0000', '-0.0000', '-0.0000', '0.4776', '-0.0000', '-0.0000', '-0.0000', '0.8486', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.0232', '0.1040', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.3750', '-0.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '0.4398', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '0.2093', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '1.0000', '0.0000', '1.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '1.0000', '1.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.2650', '0.0000', '-0.0000', '1.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '1.0000', '0.0000', '1.0000', '-0.0000', '0.3789', '-0.0000', '1.0000', '1.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.6345', '0.0000', '0.6412', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '0.7824', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '1.0000', '0.1920', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '0.5947', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.8137', '-0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '1.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.2255']\n",
      "Class 2: 46 alphas found: ['0.2828', '0.3100', '0.3231', '0.6763', '0.2974', '0.2989', '0.3313', '0.3152', '0.3248', '0.3076', '0.3313', '0.6831', '0.3208', '0.2956', '0.6730', '0.3105', '0.3226', '0.3356', '0.3178', '0.3198', '0.3363', '0.6816', '0.3196', '0.2934', '0.6857', '0.3306', '0.6785', '0.2889', '0.3280', '0.2733', '0.2977', '0.3318', '0.3226', '0.3113', '0.3233', '0.2853', '0.3091', '0.3168', '0.3223', '0.2968', '0.3044', '0.2893', '0.3255', '0.3183', '0.2957', '0.3383', '0.3219', '0.3223', '0.2956', '0.3094', '0.3307', '0.3187', '0.3184', '0.3156', '0.3205', '0.3042', '0.3206', '0.2948', '0.3198', '0.6637', '0.2805', '0.2880', '0.3222', '0.3178', '0.3161', '0.3214', '0.3291', '0.3298', '0.2743', '0.3147', '0.3159', '0.3152', '0.3068', '0.3206', '0.3271', '0.3230', '0.3126', '0.2934', '0.2931', '0.3012', '0.3359', '0.3241', '0.3364', '0.3241', '0.2822', '0.3020', '0.3345', '0.3161', '0.3039', '0.3315', '0.3247', '0.3193', '0.3191', '0.3249', '0.3036', '0.6988', '0.2932', '0.3243', '0.3274', '0.3232', '0.3024', '0.6751', '0.3022', '0.3200', '0.3014', '0.6744', '0.3279', '0.3120', '0.3028', '0.2991', '0.3313', '0.7005', '0.3269', '0.2967', '0.2983', '0.3129', '0.3189', '0.3169', '0.3051', '0.3171', '0.2996', '0.6735', '0.2978', '0.3238', '0.3129', '0.2999', '0.2873', '0.3325', '0.2599', '0.2964', '0.6915', '0.3146', '0.3335', '0.3096', '0.2809', '0.3189', '0.2883', '0.3142', '0.3076', '0.2993', '0.3281', '0.3135', '0.3317', '0.3252', '0.3107', '0.3188', '0.3220', '0.6831', '0.3193', '0.2822', '0.3117', '0.3312', '0.7024', '0.3292', '0.3230', '0.6714', '0.3256', '0.3255', '0.3015', '0.3276', '0.2998', '0.3167', '0.6859', '0.6708', '0.3160', '0.3207', '0.2878', '0.3308', '0.6693', '0.3295', '0.3078', '0.3298', '0.2950', '0.3098', '0.3078', '0.3217', '0.3228', '0.3183', '0.3032', '0.3143', '0.3184', '0.2567', '0.3002', '0.6800', '0.3107', '0.2912', '0.3206', '0.3188', '0.6674', '0.3290', '0.3198', '0.6757', '0.2932', '0.2944', '0.3061', '0.7068', '0.3159', '0.3007', '0.2818', '0.2984', '0.6767', '0.2994', '0.2662', '0.2760', '0.3175', '0.3129', '0.3273', '0.2906', '0.3041', '0.6821', '0.6810', '0.3095', '0.2841', '0.3266', '0.3049', '0.3254', '0.3438', '0.3271', '0.3053', '0.3258', '0.2960', '0.2905', '0.2788', '0.3388', '0.2790', '0.3166', '0.3240', '0.3103', '0.3229', '0.2649', '0.3223', '0.3268', '0.3183', '0.3227', '0.3058', '0.3266', '0.3266', '0.3344', '0.3079', '0.2703', '0.3030', '0.3198', '0.3133', '0.3306', '0.3012', '0.3145', '0.6767', '0.3028', '0.2828', '0.2945', '0.3156', '0.3294', '0.2915', '0.3405', '0.3172', '0.6914', '0.3197', '0.6646', '0.3298', '0.2749', '0.3168', '0.3369', '0.3045', '0.2886', '0.3257', '0.3092', '0.3044', '0.3119', '0.3255', '0.3121', '0.3354', '0.2851', '0.2975', '0.3037', '0.3344', '0.3257', '0.3010', '0.3237', '0.2968', '0.3126', '0.3222', '0.3032', '0.3199', '0.3294', '0.3266', '0.2811', '0.3295', '0.3192', '0.2952', '0.2953', '0.3085', '0.3376', '0.7157', '0.3291', '0.3227', '0.3244', '0.3195', '0.3332', '0.3601', '0.2597']\n",
      "Class 3: 46 alphas found: ['0.3243', '0.3564', '0.3713', '0.3723', '0.3406', '0.3423', '0.6194', '0.3614', '0.3737', '0.3533', '0.3813', '0.3648', '0.3684', '0.3402', '0.3757', '0.3560', '0.3708', '0.6129', '0.3651', '0.3682', '0.3871', '0.3657', '0.3668', '0.3359', '0.3604', '0.3795', '0.3693', '0.3311', '0.3772', '0.3139', '0.3409', '0.6180', '0.3707', '0.3569', '0.3724', '0.3281', '0.3555', '0.6343', '0.3699', '0.3412', '0.3508', '0.3316', '0.3739', '0.3661', '0.3400', '0.6102', '0.3701', '0.3708', '0.3385', '0.6428', '0.3805', '0.3661', '0.3655', '0.3628', '0.3686', '0.3493', '0.3685', '0.3378', '0.3669', '0.3877', '0.3213', '0.3298', '0.6301', '0.3648', '0.3626', '0.3693', '0.3784', '0.3793', '0.3139', '0.3609', '0.3628', '0.3618', '0.3516', '0.3677', '0.3756', '0.3712', '0.3589', '0.3363', '0.3372', '0.3462', '0.3864', '0.3732', '0.3866', '0.3726', '0.3231', '0.3459', '0.6147', '0.3629', '0.3483', '0.3812', '0.3727', '0.3673', '0.3661', '0.3742', '0.3488', '0.3450', '0.3358', '0.3729', '0.3768', '0.3718', '0.3467', '0.3734', '0.3462', '0.3682', '0.3452', '0.3749', '0.3767', '0.3584', '0.3474', '0.6560', '0.3806', '0.3431', '0.6230', '0.3423', '0.3429', '0.3596', '0.3663', '0.3634', '0.6476', '0.3638', '0.3433', '0.3749', '0.3419', '0.6280', '0.3587', '0.3440', '0.3291', '0.6175', '0.2980', '0.3398', '0.3548', '0.3609', '0.6162', '0.3549', '0.3217', '0.3657', '0.3301', '0.3609', '0.3526', '0.3427', '0.6222', '0.3596', '0.3817', '0.3749', '0.3586', '0.3663', '0.3700', '0.3642', '0.3663', '0.3242', '0.3576', '0.3809', '0.3408', '0.3785', '0.3711', '0.3774', '0.3744', '0.3741', '0.3458', '0.3770', '0.3436', '0.3643', '0.3610', '0.3787', '0.3624', '0.3680', '0.3300', '0.3806', '0.3808', '0.3785', '0.3538', '0.6197', '0.3378', '0.3565', '0.3538', '0.3692', '0.3708', '0.3663', '0.3473', '0.3613', '0.3655', '0.2940', '0.3439', '0.3670', '0.6436', '0.3335', '0.3679', '0.3667', '0.3832', '0.3786', '0.3669', '0.3731', '0.3374', '0.3370', '0.3528', '0.3358', '0.3624', '0.3452', '0.3230', '0.3434', '0.3712', '0.3435', '0.3073', '0.3162', '0.3647', '0.3588', '0.3759', '0.3344', '0.3485', '0.3651', '0.3665', '0.3548', '0.3255', '0.3751', '0.3495', '0.3736', '0.6037', '0.3759', '0.3498', '0.3743', '0.3403', '0.3329', '0.3201', '0.6103', '0.3213', '0.3634', '0.3724', '0.3558', '0.3707', '0.3034', '0.3700', '0.3757', '0.3663', '0.3707', '0.3509', '0.6245', '0.3749', '0.6152', '0.3528', '0.3097', '0.6503', '0.3680', '0.3595', '0.3800', '0.3454', '0.3611', '0.3713', '0.3470', '0.3248', '0.6599', '0.3620', '0.6197', '0.3338', '0.3915', '0.3638', '0.3538', '0.3670', '0.3862', '0.3793', '0.3147', '0.3634', '0.6116', '0.3496', '0.3306', '0.3752', '0.3543', '0.3493', '0.3576', '0.3751', '0.3579', '0.3856', '0.3270', '0.3407', '0.3496', '0.6149', '0.3743', '0.3447', '0.3719', '0.3398', '0.3586', '0.3706', '0.3475', '0.3675', '0.3800', '0.6242', '0.3219', '0.3788', '0.3664', '0.3380', '0.3383', '0.3548', '0.3885', '0.3274', '0.6217', '0.3707', '0.3729', '0.3665', '0.6163', '0.4150', '0.7014']\n",
      "Class 4: 46 alphas found: ['-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.3571', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.1009', '-0.0000', '0.3498', '0.0000', '-0.0000', '-0.0000', '1.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '-0.0000', '1.0000', '0.0000', '0.7860', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.9184', '1.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '1.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '0.0000', '0.0562', '-0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '1.0000', '0.7681', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0092', '0.0000', '0.8747', '-0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '0.8924', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '1.0000', '1.0000', '-0.0000', '0.0367', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.9211', '-0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '1.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0733', '0.0000', '-0.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '1.0000', '0.0912', '-0.0000', '0.8416', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '0.9124', '-0.0000', '0.0000', '1.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '-0.0000', '0.3139', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.4550', '-0.0000', '0.0000', '-0.0000', '0.5061', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '0.0178', '-0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000']\n",
      "Class 5: 46 alphas found: ['-0.0000', '0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.1312', '1.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.8948', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.2822', '0.0000', '-0.0000', '0.0000', '1.0000', '1.0000', '0.7000', '0.0000', '1.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '1.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0393', '1.0000', '0.0000', '0.8507', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '0.0129', '0.0000', '0.0000', '0.0000', '0.8090', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '0.2293', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '1.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.7739', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.5061', '1.0000', '0.0000', '-0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.4378', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.3195', '0.2664']\n",
      "Class 6: 46 alphas found: ['0.1806', '0.1980', '0.2064', '0.2068', '0.1900', '0.8090', '0.2116', '0.7987', '0.2075', '0.1965', '0.2116', '0.2024', '0.2049', '0.1888', '0.2088', '0.8017', '0.2061', '0.2144', '0.2030', '0.2043', '0.2148', '0.2034', '0.7959', '0.8126', '0.2008', '0.2112', '0.2054', '0.1845', '0.2095', '0.1745', '0.8098', '0.2119', '0.2060', '0.1988', '0.2065', '0.1823', '0.1974', '0.2023', '0.2059', '0.1896', '0.1945', '0.1848', '0.2079', '0.2033', '0.1889', '0.2161', '0.2056', '0.2059', '0.8112', '0.1976', '0.2112', '0.2036', '0.2034', '0.2016', '0.2047', '0.1943', '0.2048', '0.1883', '0.2042', '0.2148', '0.1792', '0.1839', '0.2058', '0.2030', '0.2019', '0.2053', '0.2102', '0.2107', '0.1752', '0.2010', '0.2018', '0.2013', '0.8040', '0.2048', '0.2090', '0.2063', '0.1997', '0.1874', '0.1872', '0.1924', '0.2145', '0.2070', '0.2149', '0.2070', '0.1802', '0.8071', '0.2136', '0.2019', '0.1941', '0.2117', '0.2074', '0.2039', '0.2038', '0.2076', '0.1939', '0.1924', '0.1873', '0.2071', '0.2091', '0.2064', '0.1931', '0.2076', '0.1930', '0.2044', '0.8075', '0.2080', '0.2095', '0.1993', '0.1934', '0.1911', '0.2116', '0.1913', '0.2088', '0.1895', '0.1905', '0.1999', '0.7963', '0.2024', '0.1949', '0.7974', '0.1914', '0.2085', '0.1902', '0.2068', '0.1998', '0.1916', '0.1835', '0.2124', '0.1660', '0.1893', '0.1971', '0.2010', '0.2130', '0.1977', '0.1794', '0.7963', '0.1841', '0.2007', '0.8035', '0.1912', '0.2096', '0.2002', '0.7881', '0.2077', '0.1985', '0.2036', '0.2057', '0.2024', '0.2039', '0.1803', '0.1991', '0.2115', '0.1901', '0.2103', '0.2063', '0.2099', '0.2080', '0.2079', '0.1926', '0.2092', '0.1915', '0.2023', '0.2006', '0.2103', '0.2018', '0.7952', '0.1838', '0.2113', '0.2112', '0.2105', '0.1966', '0.2106', '0.8116', '0.1979', '0.1966', '0.2055', '0.2062', '0.2033', '0.8063', '0.2007', '0.2034', '0.8360', '0.1918', '0.2044', '0.1985', '0.1860', '0.7952', '0.2036', '0.2125', '0.2102', '0.7957', '0.2071', '0.1873', '0.8120', '0.1955', '0.1873', '0.2018', '0.1920', '0.1800', '0.1906', '0.2065', '0.1913', '0.1700', '0.1763', '0.2028', '0.1998', '0.2091', '0.1856', '0.1942', '0.2030', '0.2038', '0.1977', '0.1815', '0.2086', '0.1947', '0.2078', '0.2196', '0.2089', '0.8050', '0.2081', '0.1891', '0.1855', '0.1781', '0.2164', '0.1782', '0.2022', '0.2069', '0.1982', '0.2062', '0.8308', '0.2059', '0.2087', '0.2033', '0.2062', '0.1953', '0.2086', '0.7914', '0.2136', '0.1966', '0.1726', '0.1935', '0.2043', '0.2001', '0.2112', '0.1924', '0.2009', '0.2065', '0.1934', '0.1807', '0.1881', '0.2016', '0.2104', '0.1862', '0.2175', '0.2026', '0.1971', '0.2042', '0.2143', '0.2107', '0.1756', '0.2024', '0.2152', '0.1945', '0.1844', '0.2080', '0.8025', '0.1945', '0.1992', '0.2079', '0.1993', '0.2142', '0.1821', '0.8100', '0.1940', '0.2136', '0.2080', '0.8078', '0.2067', '0.8104', '0.1997', '0.2058', '0.1937', '0.2044', '0.2104', '0.2086', '0.1796', '0.2104', '0.2039', '0.8114', '0.1886', '0.1971', '0.2156', '0.1816', '0.2102', '0.2061', '0.2072', '0.2041', '0.2128', '0.2300', '0.1659']\n",
      "Class 7: 46 alphas found: ['0.2380', '0.2610', '0.2719', '0.2725', '0.2503', '0.2516', '0.2788', '0.2653', '0.2734', '0.2589', '0.2789', '0.2668', '0.2701', '0.2488', '0.2752', '0.2613', '0.2716', '0.2825', '0.2675', '0.7308', '0.2831', '0.2680', '0.2690', '0.2470', '0.2646', '0.2783', '0.2706', '0.2432', '0.2761', '0.2300', '0.2506', '0.2793', '0.2715', '0.2620', '0.2721', '0.2402', '0.2602', '0.2667', '0.2713', '0.2499', '0.2563', '0.2435', '0.2740', '0.2680', '0.7511', '0.2847', '0.2710', '0.7287', '0.2488', '0.2604', '0.2783', '0.2683', '0.2680', '0.2657', '0.7303', '0.2561', '0.2699', '0.2481', '0.2692', '0.2831', '0.2361', '0.2424', '0.2712', '0.2675', '0.7339', '0.2706', '0.2771', '0.2776', '0.2308', '0.2649', '0.2659', '0.7347', '0.2583', '0.2699', '0.2754', '0.2719', '0.2631', '0.2470', '0.2467', '0.2536', '0.7173', '0.2728', '0.7168', '0.2728', '0.2375', '0.2542', '0.2815', '0.2660', '0.2558', '0.2790', '0.7267', '0.2687', '0.7314', '0.2735', '0.2556', '0.2535', '0.2468', '0.7270', '0.2755', '0.2720', '0.2545', '0.2735', '0.2544', '0.2694', '0.2537', '0.2741', '0.7240', '0.2626', '0.2549', '0.2518', '0.2789', '0.2521', '0.2752', '0.2498', '0.2511', '0.2634', '0.2685', '0.2667', '0.2568', '0.2670', '0.2522', '0.2748', '0.2507', '0.2726', '0.2633', '0.7476', '0.2419', '0.2799', '0.2188', '0.2495', '0.2597', '0.2648', '0.2808', '0.2606', '0.2364', '0.2685', '0.2427', '0.7355', '0.2589', '0.7481', '0.2762', '0.2639', '0.2792', '0.2738', '0.2616', '0.2684', '0.2711', '0.2667', '0.2687', '0.2375', '0.2624', '0.2788', '0.2505', '0.7229', '0.2719', '0.2766', '0.2741', '0.7260', '0.7462', '0.7243', '0.2524', '0.2666', '0.2644', '0.2771', '0.2660', '0.2699', '0.2423', '0.2785', '0.2783', '0.2774', '0.2591', '0.2776', '0.2483', '0.2608', '0.2591', '0.2708', '0.2718', '0.2679', '0.2552', '0.7355', '0.7320', '0.2161', '0.2527', '0.2693', '0.2615', '0.2451', '0.2699', '0.2683', '0.2800', '0.2770', '0.2692', '0.2729', '0.2468', '0.2478', '0.2576', '0.2468', '0.7341', '0.2531', '0.2372', '0.2512', '0.2721', '0.2520', '0.2240', '0.2323', '0.2672', '0.2633', '0.2755', '0.2446', '0.2560', '0.2676', '0.2685', '0.2605', '0.2391', '0.2749', '0.2566', '0.2739', '0.2894', '0.7247', '0.2570', '0.2742', '0.7508', '0.2445', '0.2347', '0.2852', '0.2348', '0.7335', '0.2727', '0.2612', '0.2718', '0.2230', '0.2713', '0.2751', '0.2680', '0.2717', '0.2574', '0.2749', '0.2749', '0.2814', '0.2591', '0.2275', '0.2550', '0.2692', '0.2637', '0.2783', '0.2535', '0.2647', '0.2722', '0.2549', '0.2381', '0.2479', '0.2657', '0.2773', '0.2454', '0.7134', '0.7330', '0.2598', '0.2691', '0.2823', '0.2776', '0.2314', '0.2667', '0.2836', '0.2563', '0.2430', '0.2741', '0.2602', '0.2563', '0.2625', '0.2740', '0.2627', '0.7177', '0.7600', '0.2504', '0.2557', '0.2815', '0.2741', '0.2533', '0.2724', '0.2498', '0.2631', '0.2712', '0.7448', '0.2693', '0.2773', '0.2749', '0.2366', '0.2773', '0.2687', '0.2485', '0.2485', '0.2597', '0.2841', '0.2393', '0.2770', '0.2716', '0.2731', '0.2689', '0.2805', '0.3031', '0.2186']\n",
      "Class 8: 46 alphas found: ['0.1650', '0.1809', '0.1885', '0.1889', '0.1735', '0.1744', '0.1933', '0.1839', '0.1895', '0.1795', '0.1933', '0.1849', '0.1872', '0.1725', '0.1908', '0.1812', '0.1883', '0.1958', '0.1854', '0.1866', '0.1962', '0.1858', '0.1865', '0.1712', '0.1834', '0.1929', '0.1876', '0.1686', '0.8086', '0.8406', '0.1737', '0.1936', '0.8118', '0.1816', '0.1886', '0.1665', '0.1804', '0.1849', '0.1881', '0.1732', '0.1776', '0.1688', '0.8100', '0.1858', '0.1725', '0.1974', '0.8121', '0.1881', '0.1725', '0.1805', '0.1929', '0.1860', '0.1858', '0.1842', '0.1870', '0.1775', '0.1871', '0.1720', '0.1866', '0.1963', '0.1637', '0.8320', '0.1880', '0.1854', '0.1845', '0.1876', '0.1921', '0.1924', '0.1600', '0.1836', '0.1843', '0.1839', '0.1790', '0.8129', '0.1909', '0.8115', '0.1824', '0.1712', '0.1710', '0.1758', '0.1960', '0.1891', '0.1963', '0.8109', '0.1647', '0.1762', '0.1952', '0.8156', '0.1773', '0.1934', '0.1895', '0.1863', '0.1862', '0.1896', '0.1772', '0.1758', '0.1711', '0.1892', '0.1910', '0.1886', '0.1764', '0.1896', '0.1763', '0.8133', '0.1759', '0.1900', '0.1913', '0.1820', '0.1767', '0.1746', '0.8067', '0.1748', '0.1908', '0.1731', '0.1741', '0.1826', '0.1861', '0.1849', '0.1780', '0.1851', '0.1748', '0.1905', '0.1738', '0.1889', '0.1826', '0.1750', '0.1677', '0.1940', '0.1517', '0.8270', '0.1800', '0.1836', '0.1946', '0.1806', '0.1639', '0.1861', '0.1682', '0.1833', '0.1795', '0.1747', '0.1915', '0.8171', '0.1935', '0.1898', '0.1813', '0.1860', '0.1879', '0.1849', '0.1863', '0.1647', '0.8181', '0.1932', '0.1736', '0.1921', '0.1885', '0.1918', '0.1900', '0.1899', '0.1759', '0.1911', '0.1750', '0.1848', '0.1833', '0.1921', '0.1844', '0.1871', '0.8320', '0.1930', '0.1930', '0.1923', '0.1796', '0.1924', '0.1722', '0.1808', '0.1796', '0.8123', '0.8116', '0.1857', '0.1769', '0.1834', '0.1858', '0.1498', '0.1752', '0.1867', '0.1813', '0.1699', '0.1871', '0.1860', '0.1941', '0.1920', '0.1866', '0.1892', '0.1711', '0.1718', '0.1786', '0.1711', '0.1843', '0.1754', '0.1644', '0.1741', '0.1886', '0.1747', '0.1553', '0.1611', '0.1852', '0.1826', '0.1910', '0.1696', '0.1774', '0.1855', '0.1861', '0.1806', '0.8342', '0.1906', '0.1779', '0.1899', '0.2006', '0.1909', '0.1782', '0.1901', '0.1727', '0.1695', '0.1627', '0.1977', '0.1628', '0.1847', '0.1890', '0.1811', '0.1884', '0.1546', '0.8119', '0.1907', '0.1858', '0.8117', '0.1784', '0.1906', '0.1906', '0.1951', '0.1796', '0.1577', '0.1768', '0.1866', '0.8172', '0.1929', '0.8243', '0.8165', '0.1887', '0.1767', '0.1650', '0.1719', '0.8158', '0.1922', '0.8299', '0.1987', '0.1851', '0.1801', '0.1866', '0.1957', '0.1925', '0.1604', '0.8151', '0.1966', '0.1777', '0.1684', '0.1900', '0.1804', '0.1777', '0.1820', '0.1899', '0.8179', '0.1957', '0.1664', '0.1736', '0.1772', '0.1952', '0.1900', '0.1756', '0.1889', '0.1732', '0.1824', '0.1880', '0.1769', '0.1867', '0.8078', '0.1906', '0.1640', '0.1922', '0.8138', '0.1723', '0.1723', '0.1800', '0.1970', '0.1659', '0.1920', '0.1883', '0.1893', '0.1864', '0.1944', '0.2101', '0.1515']\n",
      "Class 9: 46 alphas found: ['0.1339', '0.8532', '0.8470', '0.1533', '0.1409', '0.1416', '0.1569', '0.1493', '0.8461', '0.1457', '0.1569', '0.1501', '0.1520', '0.1400', '0.1549', '0.1470', '0.1528', '0.1590', '0.8495', '0.1515', '0.8407', '0.1508', '0.1514', '0.1390', '0.1489', '0.1566', '0.1523', '0.1368', '0.1554', '0.1294', '0.1410', '0.1572', '0.1528', '0.1474', '0.1531', '0.1351', '0.1464', '0.1500', '0.1527', '0.1406', '0.1442', '0.8630', '0.1542', '0.8492', '0.1400', '0.1602', '0.1525', '0.1527', '0.1400', '0.1465', '0.1566', '0.8490', '0.1508', '0.1495', '0.1518', '0.1441', '0.8481', '0.1396', '0.8485', '0.1593', '0.1329', '0.1364', '0.1526', '0.1505', '0.1497', '0.1522', '0.1559', '0.8438', '0.1299', '0.1491', '0.1496', '0.1493', '0.1453', '0.1519', '0.1549', '0.1530', '0.1481', '0.1390', '0.1388', '0.1427', '0.1591', '0.8465', '0.1593', '0.1535', '0.1337', '0.1430', '0.1584', '0.1497', '0.1439', '0.1570', '0.1538', '0.1512', '0.1512', '0.1539', '0.1438', '0.1427', '0.1388', '0.1536', '0.8450', '0.1531', '0.1432', '0.1539', '0.1431', '0.1516', '0.1428', '0.1542', '0.1553', '0.1478', '0.1434', '0.1417', '0.1569', '0.1419', '0.1548', '0.1405', '0.1413', '0.1482', '0.1511', '0.1501', '0.1445', '0.1502', '0.1419', '0.1546', '0.1411', '0.1534', '0.1482', '0.1420', '0.1361', '0.1575', '0.1231', '0.1404', '0.1461', '0.1490', '0.1580', '0.1466', '0.1330', '0.1511', '0.1365', '0.1488', '0.1457', '0.1418', '0.1554', '0.1485', '0.1571', '0.8460', '0.8528', '0.1510', '0.1525', '0.1501', '0.1512', '0.1337', '0.1476', '0.1569', '0.1409', '0.1559', '0.8470', '0.1557', '0.8458', '0.1542', '0.1428', '0.1552', '0.1420', '0.8500', '0.1488', '0.1559', '0.1497', '0.1519', '0.1363', '0.1567', '0.1566', '0.8439', '0.1458', '0.1562', '0.1397', '0.1468', '0.1458', '0.1524', '0.1529', '0.1507', '0.1436', '0.1489', '0.1508', '0.1216', '0.1422', '0.1515', '0.1472', '0.1379', '0.1519', '0.1510', '0.1575', '0.1558', '0.1515', '0.1536', '0.1389', '0.1394', '0.1450', '0.1389', '0.1496', '0.1424', '0.1335', '0.1414', '0.1531', '0.1418', '0.1261', '0.1307', '0.8496', '0.1482', '0.8450', '0.1376', '0.1440', '0.1506', '0.1511', '0.1466', '0.1346', '0.1547', '0.1444', '0.8459', '0.1628', '0.1549', '0.1446', '0.1543', '0.1402', '0.1376', '0.1321', '0.1605', '0.1321', '0.1500', '0.1534', '0.1470', '0.1529', '0.1255', '0.1526', '0.8452', '0.1508', '0.1529', '0.1448', '0.1547', '0.1547', '0.1584', '0.1458', '0.1280', '0.1435', '0.1515', '0.1484', '0.1566', '0.1426', '0.1490', '0.1531', '0.1434', '0.1340', '0.1395', '0.1495', '0.1560', '0.1381', '0.1613', '0.1502', '0.1462', '0.1514', '0.1589', '0.1562', '0.1302', '0.1501', '0.1596', '0.8558', '0.1367', '0.1542', '0.1464', '0.1442', '0.1477', '0.8458', '0.1478', '0.1588', '0.1350', '0.1409', '0.1439', '0.1584', '0.8458', '0.1426', '0.8467', '0.1406', '0.1481', '0.8474', '0.1436', '0.1515', '0.1560', '0.1547', '0.1331', '0.1560', '0.1512', '0.1398', '0.1399', '0.1461', '0.1599', '0.1347', '0.1559', '0.1528', '0.8464', '0.8487', '0.1578', '0.1706', '0.1230']\n",
      "Class 0: 294 alphas found: ['0.0594', '0.1553', '0.0162', '0.0487', '0.0270', '0.0764', '0.1098', '0.0165', '0.2224', '0.0970', '0.0104', '0.1577', '0.5486', '0.1318', '0.5219', '0.5573', '0.5617', '0.7284', '0.2382', '0.1184', '0.2945', '-0.0130', '0.1104', '0.0308', '0.0516', '-0.0047', '0.0206', '0.1299', '0.0654', '0.0797', '0.6361', '0.1083', '0.6088', '0.0692', '0.1769', '0.5717', '0.1000', '0.1736', '0.0176', '0.1001', '0.0459', '0.1869', '0.1305', '0.0927', '0.0642', '0.1284', '0.1777', '0.0945', '0.0246', '0.1629', '0.1157', '0.0364', '0.1136', '0.1839', '0.0924', '0.0236', '0.0938', '0.0695', '0.1395', '0.0885', '0.1729', '0.2432', '0.0835', '0.0632', '0.0538', '0.1297', '0.1719', '0.5388', '0.1105', '0.1501', '0.0783', '0.1625', '0.0585', '0.1349', '0.0748', '0.1014', '0.0577', '0.5335', '0.0374', '0.1309', '0.0942', '0.0317', '0.1011', '0.0039', '0.0800', '0.0947', '0.1376', '0.0335', '0.1814', '0.0511', '0.0209', '0.1485', '0.0535', '0.1608', '0.0394', '0.8115', '0.1143', '0.0698', '0.0426', '0.0789', '0.0366', '0.2887', '0.1717', '0.3206', '0.1382', '0.1219', '0.0877', '0.0117', '0.5400', '0.1216', '0.6791', '0.0440', '0.0499', '0.0522', '0.0774', '0.0209', '0.0462', '0.0701', '0.0743', '0.0763', '0.0249', '0.1107', '0.0837', '0.1161', '0.0802', '0.5640', '0.0815', '0.0836', '0.0175', '0.0400', '0.0822', '0.1689', '0.0722', '0.0665', '0.0744', '0.0365', '-0.0060', '0.4023', '0.1367', '0.1085', '0.1273', '0.0614', '0.8272', '0.0388', '0.1194', '0.1399', '0.1102', '0.1347', '0.2251', '0.2071', '0.1022', '0.0988', '0.1987', '0.1034', '0.0941', '0.0435', '0.0970', '0.1252', '0.1519', '0.0701', '0.0765', '0.0932', '0.1212', '0.8684', '0.0960', '0.0782', '0.0234', '0.0723', '0.7296', '0.0612', '0.0059', '0.6474', '0.1238', '0.1153', '0.0537', '0.0399', '0.0863', '0.1116', '-0.0767', '0.0485', '0.0931', '0.0116', '0.0455', '0.0347', '0.1028', '0.0587', '0.0730', '0.5459', '0.1184', '0.0606', '0.0176', '0.0270', '0.5142', '0.0320', '0.1332', '0.1416', '0.6203', '0.0872', '0.0574', '0.1601', '0.1392', '0.1891', '0.0275', '0.4507', '0.0762', '0.0362', '0.0678', '0.1530', '0.0136', '0.0304', '0.2793', '0.1098', '0.0243', '0.1146', '0.2049', '0.0482', '0.0280', '0.0236', '0.0660', '0.0453', '0.0551', '0.0974', '0.6802', '0.0711', '0.2334', '0.1384', '0.7663', '0.1283', '0.0228', '0.2372', '0.0118', '0.1131', '0.7423', '0.6060', '0.2123', '0.0415', '0.0593', '0.1110', '0.0294', '0.0623', '0.1001', '0.1048', '0.0207', '-0.0014', '0.0383', '0.0524', '0.0263', '0.1387', '0.0546', '0.1811', '0.1394', '0.0141', '0.1313', '0.1151', '0.0187', '0.2193', '0.0700', '0.7319', '0.1047', '0.0117', '0.3386', '0.0771', '0.1162', '0.0488', '0.0746', '0.1324', '0.0673', '0.1334', '0.1416', '0.0263', '0.5516', '0.1675', '0.1483', '0.1106', '0.0314', '0.1165', '0.1061', '0.0346', '0.0816', '0.1588', '0.0612', '0.0975', '0.1660', '0.1806', '0.1258', '0.1271', '0.6172', '0.0535', '0.1119', '0.5918', '0.0842', '0.1281', '0.0497', '-0.0785', '0.1146', '0.0743', '0.1745', '0.1852', '0.1437', '0.1302']\n",
      "Class 1: 294 alphas found: ['0.3236', '0.3428', '0.3708', '0.3404', '0.3421', '0.3711', '0.3793', '0.3609', '0.3719', '0.3794', '0.3631', '0.3079', '0.2987', '0.6328', '0.3380', '0.3576', '0.3512', '0.3691', '0.3636', '0.3664', '0.3849', '0.3648', '0.6172', '0.3660', '0.3598', '0.3592', '0.3425', '0.3758', '0.3797', '0.3693', '0.3699', '0.3701', '0.3534', '0.3631', '0.6313', '0.3390', '0.3728', '0.3645', '0.3385', '0.3874', '0.3693', '0.3389', '0.3745', '0.3710', '0.3546', '0.6211', '0.3647', '0.6355', '0.3380', '0.6388', '0.3663', '0.3671', '0.3886', '0.3658', '0.3854', '0.3371', '0.3295', '0.3639', '0.3617', '0.3711', '0.3768', '0.3698', '0.3849', '0.3777', '0.3755', '0.6865', '0.6401', '0.3312', '0.3617', '0.3606', '0.3713', '0.2869', '0.3511', '0.3676', '0.3670', '0.3579', '0.3666', '0.3349', '0.3847', '0.3713', '0.3713', '0.3456', '0.3831', '0.3620', '0.3477', '0.3796', '0.3716', '0.3660', '0.3722', '0.3478', '0.3449', '0.6649', '0.3715', '0.3751', '0.3704', '0.3732', '0.3708', '0.3459', '0.3668', '0.3447', '0.3730', '0.3724', '0.3754', '0.6433', '0.3464', '0.3792', '0.3427', '0.3585', '0.3394', '0.3555', '0.3578', '0.3653', '0.3628', '0.3498', '0.3630', '0.3430', '0.3422', '0.3433', '0.3809', '0.3707', '0.3388', '0.3719', '0.3711', '0.3392', '0.3535', '0.3361', '0.3603', '0.3837', '0.3822', '0.3215', '0.3650', '0.6706', '0.3520', '0.3424', '0.3759', '0.3721', '0.3506', '0.3558', '0.3650', '0.6313', '0.3741', '0.3566', '0.3793', '0.3407', '0.6479', '0.3770', '0.3471', '0.3762', '0.3814', '0.3727', '0.3727', '0.3623', '0.3729', '0.3662', '0.3451', '0.3144', '0.3627', '0.3770', '0.3332', '0.3617', '0.3671', '0.3294', '0.6391', '0.3789', '0.3752', '0.3779', '0.3376', '0.3699', '0.3718', '0.3812', '0.3629', '0.3519', '0.3681', '0.3717', '0.3696', '0.3471', '0.3645', '0.2936', '0.3264', '0.3663', '0.3555', '0.3334', '0.3548', '0.3654', '0.3915', '0.3661', '0.3714', '0.3350', '0.6313', '0.3690', '0.3290', '0.3368', '0.3500', '0.3357', '0.6876', '0.3222', '0.3414', '0.3426', '0.3672', '0.6843', '0.6423', '0.3745', '0.3401', '0.3319', '0.6325', '0.3666', '0.3480', '0.3653', '0.3323', '0.3640', '0.3811', '0.6515', '0.3742', '0.6514', '0.3723', '0.3940', '0.3689', '0.3733', '0.3388', '0.3680', '0.3622', '0.3324', '0.3186', '0.3879', '0.6809', '0.7213', '0.3707', '0.3589', '0.3677', '0.3672', '0.3699', '0.3030', '0.3479', '0.3527', '0.3741', '0.3649', '0.3695', '0.3502', '0.3524', '0.3472', '0.3759', '0.3585', '0.3336', '0.3602', '0.3669', '0.3701', '0.3466', '0.3237', '0.3377', '0.3742', '0.3611', '0.3338', '0.3898', '0.3629', '0.3534', '0.6343', '0.3842', '0.3775', '0.3896', '0.3861', '0.3480', '0.3303', '0.3557', '0.3538', '0.3486', '0.6316', '0.3570', '0.3171', '0.3261', '0.3404', '0.3472', '0.3678', '0.3830', '0.3730', '0.3444', '0.3706', '0.3578', '0.3459', '0.6338', '0.3775', '0.3710', '0.3740', '0.6787', '0.3795', '0.3531', '0.6624', '0.3528', '0.6578', '0.3867', '0.3228', '0.3770', '0.3503', '0.3424', '0.3265', '0.3766', '0.3635', '0.6306', '0.3714', '0.3655', '0.3525']\n",
      "Class 2: 294 alphas found: ['0.1177', '0.1247', '0.8651', '0.1238', '0.1244', '0.8650', '0.1380', '0.1313', '0.1353', '0.1380', '0.8679', '0.1120', '0.1086', '0.1335', '0.1229', '0.1300', '0.1277', '0.1342', '0.1322', '0.1333', '0.1400', '0.8673', '0.1392', '0.1331', '0.8692', '0.1306', '0.1246', '0.1367', '0.1381', '0.1343', '0.1345', '0.1346', '0.1285', '0.1321', '0.1341', '0.1233', '0.1356', '0.1326', '0.1231', '0.1409', '0.1343', '0.1232', '0.1362', '0.1349', '0.1290', '0.1378', '0.1326', '0.1326', '0.1229', '0.1314', '0.1332', '0.1335', '0.8587', '0.1330', '0.8598', '0.1226', '0.1198', '0.1324', '0.1315', '0.1350', '0.1370', '0.1345', '0.1400', '0.1374', '0.1366', '0.1140', '0.1309', '0.1205', '0.1315', '0.1311', '0.1350', '0.1043', '0.1277', '0.1337', '0.1335', '0.1301', '0.1333', '0.1218', '0.1399', '0.1350', '0.1350', '0.1257', '0.1393', '0.1316', '0.1265', '0.1381', '0.1351', '0.1331', '0.1354', '0.1265', '0.8746', '0.1219', '0.1351', '0.1364', '0.1347', '0.1357', '0.1349', '0.1258', '0.1334', '0.1254', '0.8643', '0.1354', '0.1365', '0.1297', '0.1260', '0.1379', '0.8754', '0.1304', '0.1234', '0.1293', '0.1301', '0.1328', '0.1319', '0.1272', '0.1320', '0.1247', '0.1245', '0.1249', '0.1385', '0.1348', '0.1232', '0.1353', '0.1350', '0.1233', '0.8714', '0.1222', '0.1310', '0.8604', '0.1390', '0.1169', '0.1327', '0.1198', '0.1280', '0.1245', '0.1367', '0.8647', '0.1275', '0.1294', '0.1327', '0.1341', '0.1361', '0.1297', '0.1379', '0.8761', '0.1281', '0.1371', '0.1262', '0.8632', '0.1387', '0.1355', '0.1356', '0.1318', '0.1356', '0.8668', '0.1255', '0.1143', '0.1319', '0.8629', '0.1212', '0.1315', '0.1335', '0.1198', '0.1313', '0.1378', '0.8635', '0.1374', '0.1228', '0.1345', '0.1352', '0.1387', '0.1320', '0.1280', '0.1339', '0.1352', '0.1344', '0.1262', '0.1326', '0.1068', '0.8813', '0.8668', '0.1293', '0.1213', '0.8710', '0.1329', '0.1424', '0.1332', '0.8649', '0.1218', '0.1341', '0.1342', '0.1196', '0.1225', '0.1273', '0.8779', '0.1136', '0.1172', '0.1242', '0.1246', '0.1335', '0.1148', '0.1301', '0.1362', '0.1237', '0.1207', '0.1337', '0.1333', '0.1266', '0.1328', '0.1209', '0.8676', '0.1386', '0.1267', '0.1361', '0.1268', '0.1354', '0.1433', '0.1341', '0.1357', '0.1232', '0.1338', '0.1317', '0.1209', '0.1159', '0.1411', '0.1161', '0.1013', '0.1348', '0.1305', '0.8663', '0.1335', '0.1345', '0.1102', '0.1265', '0.1283', '0.1361', '0.1327', '0.1344', '0.1274', '0.1282', '0.1263', '0.1367', '0.1304', '0.1213', '0.1310', '0.1334', '0.8654', '0.1261', '0.1177', '0.1228', '0.1361', '0.1313', '0.1214', '0.1418', '0.1320', '0.8715', '0.1330', '0.8603', '0.1373', '0.1417', '0.1404', '0.1265', '0.1201', '0.1293', '0.1287', '0.1268', '0.1340', '0.1298', '0.1153', '0.1186', '0.1238', '0.1263', '0.8662', '0.1393', '0.1357', '0.1252', '0.1348', '0.1301', '0.1258', '0.1332', '0.1373', '0.1349', '0.1360', '0.1168', '0.1380', '0.1284', '0.1228', '0.1283', '0.1244', '0.1406', '0.1174', '0.1371', '0.1274', '0.1245', '0.8812', '0.1369', '0.1322', '0.1343', '0.1351', '0.1329', '0.1282']\n",
      "Class 3: 294 alphas found: ['0.6003', '0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '1.0000', '0.4715', '0.0369', '0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.6378', '1.0000', '0.2119', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.4891', '1.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.3893', '0.0000', '0.0669', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '1.0000', '0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.2257', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0180', '0.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.5695', '0.2420', '0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.4422', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '0.7295', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.4935', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.5382', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0833', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.9392', '0.0059', '0.0000', '-0.0000', '0.0000', '0.4758', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000']\n",
      "Class 4: 294 alphas found: ['0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '1.0000', '0.0000', '0.4057', '0.0000', '0.0000', '0.0000', '0.0000', '0.1430', '-0.0000', '0.3491', '0.0000', '1.0000', '0.8803', '-0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.1973', '0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '1.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '0.0000', '0.0809', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.2332', '0.0000', '1.0000', '1.0000', '0.0000', '-0.0000', '0.6393', '0.0000', '-0.0000', '-0.0000', '0.0580', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '0.1244', '1.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.1776', '0.0000', '1.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '1.0000', '1.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '-0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0942', '-0.0000', '-0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '1.0000', '-0.0000', '1.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '-0.0000', '1.0000', '0.0000', '1.0000', '1.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '1.0000', '0.0000', '1.0000', '0.3615', '-0.0000', '0.6293', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '1.0000', '-0.0000', '1.0000', '0.0000', '-0.0000', '-0.0000', '0.0000', '1.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '1.0000', '0.3190', '1.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.8203', '-0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '1.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '-0.0000', '0.0000', '-0.0000', '0.0000', '0.0000', '0.0000', '1.0000', '0.0000', '-0.0000', '0.3598', '0.0000', '0.2604', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000', '0.0000']\n",
      "Class 5: 294 alphas found: ['0.8954', '0.1109', '0.1199', '0.8899', '0.1106', '0.1200', '0.1227', '0.1167', '0.1203', '0.8773', '0.1174', '0.0995', '0.0966', '0.1187', '0.1093', '0.1156', '0.1136', '0.1193', '0.1176', '0.1185', '0.1245', '0.1180', '0.1238', '0.1183', '0.1163', '0.1162', '0.8892', '0.1215', '0.1228', '0.1194', '0.1196', '0.1197', '0.1143', '0.1174', '0.1192', '0.1096', '0.1205', '0.1179', '0.1094', '0.1253', '0.1194', '0.1096', '0.1211', '0.1200', '0.1147', '0.1225', '0.1179', '0.1179', '0.1093', '0.1168', '0.1185', '0.1187', '0.1257', '0.1183', '0.1246', '0.1090', '0.1065', '0.1177', '0.1169', '0.1200', '0.1218', '0.1196', '0.1245', '0.1221', '0.1214', '0.1014', '0.1164', '0.1071', '0.8831', '0.1166', '0.1201', '0.0928', '0.1135', '0.1189', '0.1187', '0.1157', '0.1185', '0.1083', '0.1244', '0.1200', '0.1201', '0.1117', '0.1239', '0.1170', '0.8876', '0.1228', '0.1202', '0.8817', '0.1203', '0.1124', '0.1115', '0.1084', '0.1201', '0.1213', '0.8802', '0.1207', '0.1199', '0.8882', '0.1186', '0.1115', '0.1206', '0.1204', '0.1214', '0.1153', '0.1120', '0.1226', '0.1108', '0.1159', '0.1097', '0.8851', '0.1157', '0.1181', '0.8827', '0.1131', '0.1174', '0.8891', '0.1107', '0.1110', '0.1232', '0.1199', '0.1095', '0.1203', '0.1200', '0.1097', '0.1143', '0.1087', '0.8835', '0.1241', '0.1236', '0.8961', '0.1180', '0.1065', '0.1138', '0.1107', '0.1215', '0.1203', '0.1134', '0.1150', '0.1180', '0.1192', '0.1210', '0.1153', '0.1226', '0.1101', '0.1139', '0.1219', '0.8878', '0.1216', '0.1233', '0.1205', '0.1205', '0.1172', '0.1206', '0.1184', '0.1116', '0.1017', '0.1173', '0.1219', '0.1077', '0.1170', '0.1187', '0.1065', '0.1167', '0.1225', '0.1213', '0.1222', '0.1091', '0.8804', '0.1202', '0.1233', '0.1173', '0.1138', '0.1190', '0.1202', '0.1195', '0.1122', '0.1179', '0.0949', '0.1055', '0.1184', '0.1150', '0.1078', '0.1147', '0.8819', '0.1266', '0.1184', '0.1201', '0.1083', '0.1192', '0.1193', '0.8936', '0.1089', '0.1132', '0.1086', '0.1010', '0.1042', '0.1104', '0.1108', '0.1187', '0.1021', '0.1157', '0.1211', '0.1100', '0.1073', '0.1188', '0.1185', '0.1125', '0.1181', '0.1075', '0.1177', '0.1232', '0.1127', '0.8790', '0.1127', '0.1204', '0.1274', '0.8807', '0.8793', '0.1095', '0.1190', '0.1171', '0.1075', '0.1030', '0.1254', '0.1032', '0.0901', '0.1199', '0.1161', '0.1189', '0.1187', '0.1196', '0.0980', '0.1125', '0.1140', '0.1210', '0.8820', '0.1195', '0.1132', '0.8860', '0.1123', '0.1215', '0.1159', '0.1079', '0.1165', '0.8814', '0.1197', '0.8879', '0.1046', '0.1092', '0.1210', '0.1168', '0.1079', '0.1260', '0.1173', '0.1143', '0.1182', '0.1242', '0.1220', '0.1260', '0.1249', '0.1125', '0.8932', '0.8850', '0.1144', '0.1127', '0.1191', '0.8846', '0.1025', '0.1054', '0.1101', '0.1123', '0.1189', '0.1239', '0.1206', '0.1113', '0.1198', '0.1157', '0.8882', '0.1184', '0.1221', '0.1200', '0.1209', '0.1039', '0.1227', '0.1142', '0.1092', '0.1141', '0.1106', '0.1250', '0.1044', '0.1219', '0.1133', '0.8893', '0.1056', '0.1218', '0.1175', '0.1194', '0.1201', '0.1182', '0.1140']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m model_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_size\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m      8\u001b[0m other_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ceph/ssd/staff/gosl/.conda/envs/py311_ntk/lib/python3.11/site-packages/sacred/config/captured_function.py:42\u001b[0m, in \u001b[0;36mcaptured_function\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# =================== run actual function =================================\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ConfigError\u001b[38;5;241m.\u001b[39mtrack(wrapped\u001b[38;5;241m.\u001b[39mconfig, wrapped\u001b[38;5;241m.\u001b[39mprefix):\n\u001b[0;32m---> 42\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# =========================================================================\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wrapped\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/ceph/ssd/staff/gosl/src/ntk-robust/exp_ntk_hyperparam.py:220\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(data_params, model_params, verbosity_params, other_params, seed, _run)\u001b[0m\n\u001b[1;32m    218\u001b[0m     X_trn \u001b[38;5;241m=\u001b[39m X[idx_known, :]\n\u001b[1;32m    219\u001b[0m     idx_known_labeled \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mnonzero(np\u001b[38;5;241m.\u001b[39misin(idx_known, idx_labeled))[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m#actually is just 0 to len(idx_labeled)\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m ntk \u001b[38;5;241m=\u001b[39m \u001b[43mNTK\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_trn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_trn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mA_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43midx_trn_labeled\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx_known_labeled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx_trn_split\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlearning_setting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlearning_setting\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpred_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpred_method\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mregularizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mregularizer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43msolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msolver\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43malpha_tol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43malpha_tol\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m y_pred, ntk_test \u001b[38;5;241m=\u001b[39m ntk(idx_labeled\u001b[38;5;241m=\u001b[39midx_trn_split, \n\u001b[1;32m    232\u001b[0m                        idx_test\u001b[38;5;241m=\u001b[39midx_val_split,\n\u001b[1;32m    233\u001b[0m                        y_test\u001b[38;5;241m=\u001b[39my, X_test\u001b[38;5;241m=\u001b[39mX, A_test\u001b[38;5;241m=\u001b[39mA, \n\u001b[1;32m    234\u001b[0m                        return_ntk\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    235\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39maccuracy(y_pred, y[idx_val_split])\n",
      "File \u001b[0;32m/ceph/ssd/staff/gosl/src/ntk-robust/src/models/ntk.py:123\u001b[0m, in \u001b[0;36mNTK.__init__\u001b[0;34m(self, model_dict, X_trn, A_trn, n_classes, idx_trn_labeled, y_trn, learning_setting, pred_method, regularizer, bias, append_dimension, solver, alpha_tol, device, dtype, print_alphas)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_alphas \u001b[38;5;241m=\u001b[39m print_alphas\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msvm\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msvm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_svm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_trn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_trn_labeled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m pred_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkrr\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/ceph/ssd/staff/gosl/src/ntk-robust/src/models/ntk.py:310\u001b[0m, in \u001b[0;36mNTK.fit_svm\u001b[0;34m(self, X, A, y, idx_trn_labeled)\u001b[0m\n\u001b[1;32m    308\u001b[0m     A \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    309\u001b[0m     b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 310\u001b[0m     alphas_, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mQPFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ml\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    311\u001b[0m alphas\u001b[38;5;241m.\u001b[39mappend(alphas_[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    312\u001b[0m alphas_str \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00malpha\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.04f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m alpha \u001b[38;5;129;01min\u001b[39;00m alphas_[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[0;32m/ceph/ssd/staff/gosl/.conda/envs/py311_ntk/lib/python3.11/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/ceph/ssd/staff/gosl/.conda/envs/py311_ntk/lib/python3.11/site-packages/proxsuite/torch/qplayer.py:163\u001b[0m, in \u001b[0;36mQPFunction.<locals>.QPFunctionFn.forward\u001b[0;34m(ctx, Q_, p_, A_, b_, G_, l_, u_)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(ctx\u001b[38;5;241m.\u001b[39mvector_of_qps\u001b[38;5;241m.\u001b[39msize()):\n\u001b[0;32m--> 163\u001b[0m         \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_of_qps\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nBatch):\n\u001b[1;32m    166\u001b[0m     zhats[i] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ctx\u001b[38;5;241m.\u001b[39mvector_of_qps\u001b[38;5;241m.\u001b[39mget(i)\u001b[38;5;241m.\u001b[39mresults\u001b[38;5;241m.\u001b[39mx)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_params[\"regularizer\"] = 1\n",
    "model_params[\"solver\"] = \"qplayer_one_vs_all\"\n",
    "model_params[\"alpha_tol\"] = 1e-4\n",
    "model_params[\"bias\"] = False\n",
    "data_params[\"learning_setting\"] = \"transductive\"\n",
    "model_params[\"pred_method\"] = \"svm\"\n",
    "model_params[\"cache_size\"] = 10000\n",
    "other_params[\"device\"] = \"cpu\"\n",
    "run(data_params, model_params, verbosity_params, other_params, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'trn_acc': 0.8416666388511658,\n",
       " 'val_acc_l': [0.8666666746139526,\n",
       "  0.699999988079071,\n",
       "  0.7666666507720947,\n",
       "  0.8666666746139526],\n",
       " 'test_acc': 0.6720730066299438,\n",
       " 'trn_min_ypred': -1.0132856369018555,\n",
       " 'trn_max_ypred': -0.92774498462677,\n",
       " 'trn_min_ntkunlabeled': 0.017343977865629755,\n",
       " 'trn_max_ntkunlabeled': 0.3979579377862144,\n",
       " 'val_cond': [29.28596287702048,\n",
       "  28.50047210021289,\n",
       "  29.87229510654015,\n",
       "  30.21689120506808],\n",
       " 'val_min_ypred': [-1.0080808401107788,\n",
       "  -1.009164810180664,\n",
       "  -1.0047053098678589,\n",
       "  -1.0055309534072876],\n",
       " 'val_max_ypred': [-0.9614898562431335,\n",
       "  -0.9390972256660461,\n",
       "  -0.9678471088409424,\n",
       "  -0.9638925194740295],\n",
       " 'val_min_ntklabeled': [0.017790753156182348,\n",
       "  0.0174271389738226,\n",
       "  0.017426028088272472,\n",
       "  0.0175991252872458],\n",
       " 'val_max_ntklabeled': [0.4699480765241749,\n",
       "  0.4594507217073751,\n",
       "  0.4699480765241749,\n",
       "  0.48532874339337995],\n",
       " 'val_min_ntkunlabeled': [0.01655318783843405,\n",
       "  0.01655318783843405,\n",
       "  0.01655318783843405,\n",
       "  0.01655318783843405],\n",
       " 'val_max_ntkunlabeled': [0.5257314784867956,\n",
       "  0.5257314784867956,\n",
       "  0.5257314784867956,\n",
       "  0.5257314784867956],\n",
       " 'test_min_ypred': -1.0247138738632202,\n",
       " 'test_max_ypred': -0.886574923992157,\n",
       " 'test_min_ntklabeled': 0.017426028088272472,\n",
       " 'test_max_ntklabeled': 0.4699480765241749,\n",
       " 'test_min_ntkunlabeled': 0.01655318783843405,\n",
       " 'test_max_ntkunlabeled': 0.5257314784867956,\n",
       " 'test_cond': 38.78955604859958}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params[\"regularizer\"] = 0.1\n",
    "model_params[\"pred_method\"] = \"svm\"\n",
    "model_params[\"cache_size\"] = 1000\n",
    "data_params[\"dataset\"] = \"pubmed\"\n",
    "other_params[\"device\"] = 0\n",
    "seed = 0\n",
    "data_params[\"specification\"][\"seed\"] = seed\n",
    "run(data_params, model_params, verbosity_params, other_params, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: ['0.86', '0.87', '0.85', '0.86', '0.78', '0.79', '0.85', '0.84', '0.82', '0.82']\n",
      "Accuracy Mean: 0.8334507048130035\n",
      "Accuracy Std: 0.02933545175833569\n",
      "Min y_pred: ['-2.01', '-1.80', '-1.93', '-1.87', '-2.13', '-2.22', '-1.89', '-2.01', '-1.89', '-1.98']\n",
      "Max y_pred: ['2.04', '1.75', '2.08', '2.14', '2.47', '2.29', '2.01', '1.82', '2.00', '2.06']\n",
      "Min NTK_labeled: ['3.52', '3.30', '2.93', '3.29', '2.95', '3.51', '3.47', '1.89', '2.73', '2.61']\n",
      "Max NTK_labeled: ['142.09', '168.08', '132.09', '110.09', '72.59', '108.09', '104.09', '324.07', '74.09', '139.80']\n",
      "Min NTK_unlabeled: ['2.64', '3.17', '2.63', '2.63', '3.35', '2.63', '3.07', '3.23', '2.33', '3.04']\n",
      "Max NTK_unlabeled: ['47.54', '39.93', '38.96', '41.18', '47.54', '47.08', '33.10', '74.87', '39.56', '43.30']\n",
      "Condition: ['26316', '13169', '25396', '25245', '25365', '16955', '16290', '13427', '15812', '27280']\n"
     ]
    }
   ],
   "source": [
    "model_params[\"regularizer\"] = 0.1\n",
    "data_params[\"dataset\"] = \"cora_ml\"\n",
    "model_params[\"pred_method\"] = \"svm\"\n",
    "n_seed_l = None\n",
    "verbosity_params[\"debug_lvl\"] = \"warning\"\n",
    "n_seeds = 10\n",
    "run_exp(n_seeds, data_params, model_params, verbosity_params, other_params, n_seed_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: ['0.78', '0.76', '0.65', '0.74', '0.72', '0.66', '0.77', '0.73', '0.72', '0.77']\n",
      "Accuracy Mean: 0.731424230337143\n",
      "Accuracy Std: 0.04095614945257118\n",
      "Min y_pred: ['-1.02', '-1.02', '-1.01', '-1.02', '-1.02', '-1.02', '-1.01', '-1.03', '-1.02', '-1.01']\n",
      "Max y_pred: ['-0.90', '-0.92', '-0.90', '-0.91', '-0.90', '-0.86', '-0.92', '-0.85', '-0.85', '-0.89']\n",
      "Min NTK_labeled: ['0.02', '0.02', '0.02', '0.02', '0.02', '0.02', '0.02', '0.02', '0.02', '0.02']\n",
      "Max NTK_labeled: ['0.50', '1.03', '0.66', '0.40', '0.96', '1.13', '0.62', '0.83', '0.59', '0.82']\n",
      "Min NTK_unlabeled: ['0.02', '0.02', '0.02', '0.02', '0.02', '0.02', '0.02', '0.02', '0.02', '0.02']\n",
      "Max NTK_unlabeled: ['0.46', '0.33', '0.27', '0.36', '0.27', '0.73', '0.25', '0.31', '0.31', '0.33']\n",
      "Condition: ['40', '47', '40', '40', '41', '44', '40', '41', '41', '43']\n"
     ]
    }
   ],
   "source": [
    "model_params[\"regularizer\"] = 0.1\n",
    "data_params[\"dataset\"] = \"pubmed\"\n",
    "model_params[\"pred_method\"] = \"svm\"\n",
    "n_seed_l = None\n",
    "n_seeds = 10\n",
    "other_params[\"device\"] = \"cpu\"\n",
    "verbosity_params[\"debug_lvl\"] = \"warning\"\n",
    "run_exp(n_seeds, data_params, model_params, verbosity_params, other_params, n_seed_l=n_seed_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ceph/ssd/staff/gosl/.conda/envs/py311_ntk/lib/python3.11/site-packages/torch_geometric/data/in_memory_dataset.py:157: UserWarning: It is not recommended to directly access the internal storage format `data` of an 'InMemoryDataset'. If you are absolutely certain what you are doing, access the internal storage via `InMemoryDataset._data` instead to suppress this warning. Alternatively, you can access stacked individual attributes of every graph via `dataset.{attr_name}`.\n",
      "  warnings.warn(msg)\n",
      "/ceph/ssd/staff/gosl/src/ntk-robust/exp_ntk.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X = torch.tensor(X, dtype=other_params[\"dtype\"], device=device)\n",
      "/ceph/ssd/staff/gosl/src/ntk-robust/exp_ntk.py:163: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  A = torch.tensor(A, dtype=other_params[\"dtype\"], device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of samples\n",
      " - labeled: 140 \n",
      " - val: 140 \n",
      " - test: 273 \n",
      " - unlabeled: 2155\n",
      "number of samples\n",
      " - labeled: 140 \n",
      " - val: 140 \n",
      " - test: 273 \n",
      " - unlabeled: 2155\n",
      "Accuracy: ['0.86', '0.84']\n",
      "Min y_pred: ['-0.61', '-0.68']\n",
      "Max y_pred: ['1.54', '2.93']\n",
      "Min NTK_labeled: ['0.55', '0.78']\n",
      "Max NTK_labeled: ['41.00', '53.00']\n",
      "Min NTK_unlabeled: ['0.52', '0.64']\n",
      "Max NTK_unlabeled: ['28.50', '28.00']\n",
      "Condition: ['931', '939']\n"
     ]
    }
   ],
   "source": [
    "model_params[\"regularizer\"] = 1\n",
    "data_params[\"dataset\"] = \"cora\"\n",
    "model_params[\"pred_method\"] = \"krr\"\n",
    "n_seeds = 2\n",
    "run_exp(n_seeds, data_params, model_params, verbosity_params, other_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311_ntk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
